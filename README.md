# ETL Pipeline with Airflow, dbt, and MySQL

## Overview

This project demonstrates how to build an **ETL (Extract, Transform, Load) pipeline** using **Apache Airflow**, **dbt (data build tool)**, and **MySQL**. The pipeline automates the extraction of raw data, transformation using dbt, and loading into a MySQL database for further analysis.

## Features

- **Automated Data Extraction**: Fetch data from CSV, APIs, or databases.
- **Data Transformation with dbt**: Clean, enrich, and model data for analytics.
- **Airflow Orchestration**: Schedule and monitor the pipeline execution.
- **MySQL Database Integration**: Store raw and processed data efficiently.
- **Docker Support**: Containerized deployment for scalability.

## ETL Pipeline Flowchart

Below is a flowchart representing the ETL pipeline:

![ETL Pipeline Flow](images/etl_pipeline.png)
```mermaid
graph TD;
    A[Extract Data] -->|Raw Data| B[Load into MySQL Staging];
    B -->|Transform with dbt| C[dbt Models];
    C -->|Cleaned & Modeled Data| D[Load into MySQL Final Tables];
    D -->|Analytics & Reporting| E[BI Dashboard];
    D -->|Data Storage| F[Warehouse];
```

## File Structure

```
etl_project/
â”‚â”€â”€ airflow/                # Airflow-related files
â”‚   â”‚â”€â”€ dags/               # DAGs directory
â”‚   â”‚   â”œâ”€â”€ etl_pipeline.py # Airflow DAG for ETL
â”‚   â”‚â”€â”€ config/             # Airflow configuration
â”‚   â”‚   â”œâ”€â”€ airflow.cfg     # Airflow config file
â”‚   â”‚â”€â”€ logs/               # Airflow logs
â”‚   â”‚â”€â”€ plugins/            # Custom plugins if needed
â”‚â”€â”€ dbt_project/            # dbt project files
â”‚   â”‚â”€â”€ models/             # SQL models
â”‚   â”‚   â”œâ”€â”€ raw/            # Raw tables (staging)
â”‚   â”‚   â”œâ”€â”€ transformed/    # Transformed tables
â”‚   â”‚   â”œâ”€â”€ final/          # Final models for reporting
â”‚   â”‚   â”œâ”€â”€ transform.sql   # Sample transformation SQL
â”‚   â”‚â”€â”€ seeds/              # Static data for dbt
â”‚   â”‚â”€â”€ tests/              # dbt tests
â”‚   â”‚â”€â”€ dbt_project.yml     # dbt project configuration
â”‚   â”‚â”€â”€ profiles.yml        # dbt MySQL connection settings
â”‚â”€â”€ scripts/                # Python scripts for ETL
â”‚   â”‚â”€â”€ extract.py          # Extract data from source
â”‚   â”‚â”€â”€ load.py             # Load data into MySQL
â”‚   â”‚â”€â”€ transform.py        # Transformation logic
â”‚â”€â”€ data/                   # Raw data storage
â”‚   â”‚â”€â”€ input/              # Input CSVs or JSONs
â”‚   â”‚â”€â”€ output/             # Processed data
â”‚â”€â”€ docker/                 # Docker-related files (if using)
â”‚   â”‚â”€â”€ Dockerfile          # Airflow & dbt setup
â”‚   â”‚â”€â”€ docker-compose.yml  # Multi-container setup
â”‚â”€â”€ config/                 # Configuration files for MySQL, dbt, etc.
â”‚   â”œâ”€â”€ db_config.json      # MySQL credentials
â”‚   â”œâ”€â”€ airflow_env.env     # Airflow environment variables
â”‚â”€â”€ logs/                   # Log files for debugging
â”‚â”€â”€ README.md               # Project documentation
```

## Installation

### **Prerequisites**

- Python 3.11.4

- MySQL installed and running

- Apache Airflow installed

- dbt-mysql installed

- Docker (optional)

### **Setup Instructions**

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/etl_project.git
   cd etl_project
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Configure MySQL connection in `config/db_config.json`:
   ```json
   {
     "host": "localhost",
     "user": "root",
     "password": "your_password",
     "database": "etl_db"
   }
   ```
4. Initialize Airflow:
   ```bash
   airflow db init
   airflow webserver --port 8080 &
   airflow scheduler &
   ```
5. Set up dbt:
   ```bash
   dbt init dbt_project
   cd dbt_project
   dbt run
   ```
6. Run the ETL pipeline:
   ```bash
   airflow dags trigger etl_pipeline
   ```

## Usage

- Monitor the pipeline in the Airflow UI (`http://localhost:8080`).
- Check transformed data in MySQL:
  ```sql
  SELECT * FROM transformed_table;
  ```
- Modify `dbt_project/models/transform.sql` to adjust transformations.

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature-name`)
3. Commit your changes (`git commit -m "Add feature"`)
4. Push to the branch (`git push origin feature-name`)
5. Open a Pull Request

## License

This project is licensed under the MIT License.

ðŸš€ **Now your ETL pipeline is ready!** ðŸš€